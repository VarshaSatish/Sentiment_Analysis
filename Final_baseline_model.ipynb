{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"baseline_model_2.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vhGdsjSIvaQ2"},"source":["---\n","> SENTIMENT ANALYSIS OF MOVIE REVIEWS USING LSTMS\n","---"]},{"cell_type":"code","metadata":{"id":"5wGJQYmDPWW7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"26916819-83be-4b75-fe5c-bedfa9f8822e"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dOq8VFX06LV8"},"source":["---\n","> The reference for this particular code was taken from \n","https://github.com/bentrevett/pytorch-sentiment-analysis.\n","> Neccessary changes were done and the changes have been mentioned as comments in the code blocks\n","---"]},{"cell_type":"markdown","metadata":{"id":"zpIkEGFfjzRS"},"source":["--- \n","> DATA PREPARATION\n","\n","---\n","\n",">* Field defines how the data must be processed\n","* TEXT field splits the string into discrete token using 'spacy' tokenizer.\n","* LABEL is defined by a LabelField, a special subset of the Field class specifically used for handling labels\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"rr7WpnnTXPoq"},"source":["import torch\n","from torchtext import data\n","from torchtext import datasets\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DevAWSy5kmN8"},"source":["> Using torchtext, we can automatically download the IMDb dataset and split it into the canonical train/test splits using torchtext.datasets objects."]},{"cell_type":"code","metadata":{"id":"o5t-4qwrXPox","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c7a76b93-c500-4547-a9d1-31871895ad2d"},"source":["from torchtext import datasets\n","\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 22.1MB/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lryWfm4ak6C9"},"source":["\n","---\n","> A random seed is given to to the random_state argument, to ensure the same train/validation split each time.\n","---"]},{"cell_type":"code","metadata":{"id":"1880DMT6XPoz"},"source":["import random\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2vYxrnewlC40"},"source":["---\n","> The following builds the vocabulary, only keeping the most common max_size tokens using 'glove.6B.100d' pretrained model.\n","---"]},{"cell_type":"code","metadata":{"id":"0FOodmD4XPo2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e189adb-9596-4463-e21c-4f160bc2e025"},"source":["MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = \"glove.6B.100d\", \n","                 unk_init = torch.Tensor.normal_)\n","\n","LABEL.build_vocab(train_data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n","100%|█████████▉| 398113/400000 [00:16<00:00, 24449.10it/s]"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"BUDdmxaylpCK"},"source":["\n","\n","---\n","\n","\n","> Preparing iterators of data\n","\n","---\n","\n","\n",">*  BucketIterator is a type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n","* torch.cuda.is_available() is used to see if GPU is available to place tensors on GPU.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"ha1u6lg6XPo5"},"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ox_w2TIWmNVW"},"source":["\n","\n","---\n","> BUILDING THE MODEL\n","\n","---\n",">* Within the __ init __ an embedding layer, our RNN, and a linear layer are defined. All layers have their parameters initialized to random values, unless explicitly specified.\n","* The embedding layer is used to transform sparse one-hot vector into a dense embedding vector\n","* The RNN layer takes in dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n","* The linear layer takes the final hidden state and feeds it through a fully connected layer, $f(h_T)$, transforming it to the correct output dimension.\n","\n","> * Forward method is called to feed samples into the model.\n","* The input batch is passed through the embedding layer to get embedded, which gives us a dense vector representation of our sentences. embedded is a tensor of size.\n","* embedded is then fed into the RNN, which returns 2 tensors - output and hidden state.\n","* The last hidden state is fed to linear layer to produce a prediction.\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"ris0kM4VvVwv"},"source":["# The model used in the original GitHub code was modified for separate functions when no. of LSTM layers is 1 or more. \n","# Also a sigmoid layer was added to the original model. \n","\n","import torch.nn as nn\n","\n","class LSTM(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","\n","        self.num_layers = n_layers\n","        \n","        self.lstm = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        \n","        self.fc = nn.Linear(hidden_dim * n_layers, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","        \n","    def forward(self, text, text_lengths):\n","\n","        text_lengths = text_lengths.cpu()\n","\n","        if self.num_layers == 1:\n","\n","           embedded = self.embedding(text)\n","           \n","           #pack sequence\n","           packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n","        \n","           packed_output, (hidden, cell) = self.lstm(packed_embedded)\n","\n","           hidden =  hidden[-1,:,:]\n","\n","           out = self.sigmoid((self.fc(hidden)))\n","        \n","        if self.num_layers > 1:\n","          \n","          embedded = self.dropout(self.embedding(text))\n","          \n","          #pack sequence\n","          packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n","        \n","          packed_output, (hidden, cell) = self.lstm(packed_embedded)\n","\n","          hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","\n","          out = self.sigmoid((self.fc(hidden)))\n","             \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDit_TNgpXsD"},"source":["---\n",">Instance of a class is created in the following block\n","---\n"]},{"cell_type":"code","metadata":{"id":"EYwWvLLZXPo_"},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 1\n","BIDIRECTIONAL = False\n","DROPOUT = 0.0\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = LSTM(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SPdvQDYaqSC2"},"source":["---\n","> The function below counts the number of parameters the model has.\n","---"]},{"cell_type":"code","metadata":{"id":"oVyEEHeRXPpC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f5f497a-9454-4a93-d15b-0b0b851f4a1c"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 2,867,049 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LUJUnhKQLE1W"},"source":["---\n","> The below code blocks are used to load the pretrained weights from 'glove100d' word embeddings into the embedding layer of our model.\n","---"]},{"cell_type":"code","metadata":{"id":"YnnrVd3vXPpF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d7f701f-43f9-415a-dda6-058e23d0ead7"},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","\n","print(pretrained_embeddings.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([25002, 100])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZDgdbtrrXPpI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f274775-dcd8-4101-8847-3f0185de4590"},"source":["model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n","        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.5302, -0.8394,  0.3944,  ..., -0.6926, -0.1440,  0.2929],\n","        [-0.2146,  0.6712,  0.3821,  ...,  0.4095,  0.7454,  0.0046],\n","        [-0.3202, -0.1139,  0.4597,  ...,  0.5334, -0.0947,  0.3415]])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"UxeDzY2fXPpL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"64eb17ec-e76f-4248-e0e7-7429388eca84"},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.5302, -0.8394,  0.3944,  ..., -0.6926, -0.1440,  0.2929],\n","        [-0.2146,  0.6712,  0.3821,  ...,  0.4095,  0.7454,  0.0046],\n","        [-0.3202, -0.1139,  0.4597,  ...,  0.5334, -0.0947,  0.3415]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iijJF0FrqeDa"},"source":["---\n",">Optimizer is the algorithm used to update the parameters in the module. ADAM optimiser is used here.\n","---"]},{"cell_type":"code","metadata":{"id":"OT2akwPJXPpP"},"source":["# In the original code, SGD was used, here we have used ADAM optimizer.\n","\n","import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5QhKeQVqs_q"},"source":["---\n","> Loss function used is Binary Cross Entropy loss\n","> We load the model to device(GPU if available)\n","---"]},{"cell_type":"code","metadata":{"id":"ZHK3O4jtXPpS"},"source":["# In the original code, BCE with Sigmoid Loss was used. Here we sre using just the BCE.\n","\n","criterion = nn.BCELoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iBJUL8FbrB3o"},"source":["---\n","> The below function calculates the accuracy.\n","---"]},{"cell_type":"code","metadata":{"id":"XatSMHZuXPpV"},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round((preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71VLuJ33rGSW"},"source":["---\n",">MODEL TRAINING\n","---\n","\n","> * model.train() puts the model to train mode.\n","* zero_grad() is used to zero the gradients.\n","* batch.text is the batches of sentences fed to the model.\n","* loss.backward() is used to calculate the gradient of each parameter. \n","* optimizer.step() is used to update the parameters using gradients and optimizer algorithm.\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"VvQZWpTCXPpX"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        \n","        text, text_lengths = batch.text\n","        \n","        predictions = model(text, text_lengths).squeeze(1)\n","        \n","        loss = criterion(predictions, batch.label)\n","        \n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FKIX8CmzsvxK"},"source":["---\n",">* model.eval() puts the model in \"evaluation mode\", turning off dropout.\n","* No parameter update happens in evaluation mode.\n","\n","---"]},{"cell_type":"code","metadata":{"id":"EmLjRI6lXPpa"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            text, text_lengths = batch.text\n","            \n","            predictions = model(text, text_lengths).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tx8m4tUyXPpe"},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIaeentaXPpg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"26467eca-6a79-4f10-866b-eacd0e8625b7"},"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), './drive/My Drive/AML_assignments/AML_2/Models/baseline_1.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.170 | Train Acc: 94.03%\n","\t Val. Loss: 0.369 |  Val. Acc: 86.69%\n","Epoch: 02 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.118 | Train Acc: 96.17%\n","\t Val. Loss: 0.372 |  Val. Acc: 86.62%\n","Epoch: 03 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.081 | Train Acc: 97.58%\n","\t Val. Loss: 0.441 |  Val. Acc: 87.15%\n","Epoch: 04 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.048 | Train Acc: 98.78%\n","\t Val. Loss: 0.531 |  Val. Acc: 87.26%\n","Epoch: 05 | Epoch Time: 0m 9s\n","\tTrain Loss: 0.036 | Train Acc: 99.11%\n","\t Val. Loss: 0.558 |  Val. Acc: 86.71%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pn_qERxhtPWl"},"source":["---\n",">TESTING THE MODEL\n","---\n",">We load the model we saved while training and test on reviews that the model hasn't seen before.\n","---"]},{"cell_type":"code","metadata":{"id":"YQyjFmNUXPpi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f815537f-ebf1-414e-c240-555e56832422"},"source":["model.load_state_dict(torch.load('./drive/My Drive/AML_assignments/AML_2/Models/baseline_1.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss: 0.407 | Test Acc: 85.03%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Ih2XHkXXPpl"},"source":["import spacy\n","nlp = spacy.load('en')\n","\n","def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    length = [len(indexed)]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    length_tensor = torch.LongTensor(length)\n","    prediction = (model(tensor, length_tensor))\n","    return prediction.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGYgd2fMXPpn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0311f63f-216b-4845-d25e-78cf69db8ffa"},"source":["pred = predict_sentiment(model, \"Boring script\")\n","print(pred)\n","if pred >= 0.5:\n","  print('positive review')\n","else : \n","  print('negative review')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.007808869704604149\n","negative review\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XhAd73LXXPpq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b90bcf0e-9cfa-4e5c-c562-ee47fcfb1345"},"source":["pred = predict_sentiment(model, \"Great acting\")\n","print(pred)\n","if pred >= 0.5:\n","  print('positive review')\n","else : \n","  print('negative review')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9678888916969299\n","positive review\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"isxO8Dan6DmH"},"source":["review_new =  \"The extreme color palette and overdone scenes of gore and nudity feel like a desparate (and unfortunately unsuccessful) attempt to divert attention from the acting, plot, and dialog.\"\n","print(\"Review : \",review_new)\n","pred = predict_sentiment(model,review_new)\n","print(pred)\n","if pred >= 0.5:\n","  print('positive review')\n","else : \n","  print('negative review')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fdUgma7S6Euo"},"source":["review_new = \"A truly unique movie unlike any I've seen - the offbeat humor is entertaining and original, the story never predictable, and the carefully blended soundtrack makes for a strangely cinematic experience.\"\n","print(\"Review : \",review_new)\n","pred = predict_sentiment(model, review_new)\n","print(pred)\n","if pred >= 0.5:\n","  print('positive review')\n","else : \n","  print('negative review')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pl0vgHK-58hX"},"source":["### **Results :**\n","We have used different specifications for our LSTM model and trained those models using IMDB dataset. We have used pretrained model for word embedding. \n","The results sheet is shared along with this notebook is discussed here.\n","The Results are explained in the following points, \n","\n","1.   First result is a simple single layer LSTM model with unidirectional and with 0% dropout. We've used ADAM optimizer as well as BCE loss. Training accuracy at the end of 5 epochs is 92.94% and validation accuracy will be 87.35%. Test accuracy is 76.17%. \n","2.   For the second result, number of LSTM layers are increased to 2. All the other parameter remains same we can expect that the accuracy will increase as network becomes deep. The different accuracies after 5 epochs are as follows,\n","Train Acc: 95.66%, Validation Acc: 87.77%, Test Acc: 84.24%. \n","3.   In 3rd result, we've used dropout of 30% with two LSTM layers same as 2nd iteration. Here our training accuracy decreased but Validation and Testing accuracies increased. The Dropout will make an impression of ensembling and hence validation and test accuracies will improve. The accuracies are as follows, Train Acc: 90.62%, Val. Acc: 88.72%, Test Acc: 87.51%.\n","4.   In fourth iteration we've increased the dropout percentage to 60% and as dropout percentage increases training and validation accuracies will increase. The network will be able to train properly as the higher dropout will remove biasing in the network. Still we can see slight reduction in testing accuracy. The accuracies are as follows, Train Acc: 92.70%, Val. Acc: 89.23%, Test Acc: 86.87%.\n","5.   In 5th iteration, we've used bidirectional LSTM. Here gradient flow from future to past and also from past to future. The testing results will improve as we're using both future and past input for training. The accuracies are as follows, Train Acc: 85.94%,  Val. Acc: 84.55%, Test Acc: 81.31%.\n","6.   In 6th iteration, we've used GRU and it has two gates instead of 3 gates as we can see in LSTM. GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. Hence we can see in the sheet the number of parameters used in GRU are less and their training and validation accuracies are more better than LSTM. The accuracies are as follows, Train Acc: 97.13%, Val. Acc: 89.16%, Test Acc: 78.54%.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]}