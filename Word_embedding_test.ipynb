{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word_embedding_test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XiJe6nw-wZei"},"source":["This notebook is to test the word embeddings for analogy tasks. This is done by taking the L2 norm of the difference of actual value and the predicted value. The lower the L2 norm is, the better."]},{"cell_type":"code","metadata":{"id":"d4oJP1vKvKG8","outputId":"f4660531-9929-45c7-d029-1178f65aecc1","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UK7f34EAvW5G","outputId":"c0c81fb8-35dd-4543-e8ca-4b38c5e9f654","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install bcolz --quiet"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting bcolz\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 3.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.18.5)\n","Building wheels for collected packages: bcolz\n","  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bcolz: filename=bcolz-1.2.1-cp36-cp36m-linux_x86_64.whl size=2668999 sha256=85654e2ab275eccdac208716f3d29b9af64b5cd912afb204a84bc24fa6bb2902\n","  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n","Successfully built bcolz\n","Installing collected packages: bcolz\n","Successfully installed bcolz-1.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_m8RbQh3vaN_"},"source":["import bcolz\n","import numpy as np\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WtXh23OGvcHb"},"source":["# Making Dictionary from carray for 50 length embedding\n","\n","vectors = bcolz.open(f'./drive/My Drive/AML_2/6B.50d.dat')[:]\n","words = pickle.load(open(f'./drive/My Drive/AML_2/6B.50_words.pkl', 'rb'))\n","word2idx = pickle.load(open(f'./drive/My Drive/AML_2/6B.50_idx.pkl', 'rb'))\n","\n","myDict_50 = {w: vectors[word2idx[w]] for w in words}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ApelbaEkp179"},"source":["# Making Dictionary from carray for 200 length embedding\n","\n","vectors_200 = bcolz.open(f'./drive/My Drive/AML_2/6B.200d.dat')[:]\n","words_200 = pickle.load(open(f'./drive/My Drive/AML_2/6B.200_words.pkl', 'rb'))\n","word2idx_200 = pickle.load(open(f'./drive/My Drive/AML_2/6B.200_idx.pkl', 'rb'))\n","\n","myDict_200 = {w: vectors_200[word2idx_200[w]] for w in words_200}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fz0m6Jak59TJ"},"source":["Here, we have taken a weighted L2 norm to effectively compare the effect of embedding vector length on the analogy task. For this, we divide the actual norm value by 250 (200 + 50) and multiply by the length corresponding to which we are trying to find the L2 norm"]},{"cell_type":"code","metadata":{"id":"JciQRqxTvfRs"},"source":["# Method for Taking L2 norm of difference of 2 word embeddings(weighted L2 norm)\n","\n","def takeL2norm(vec1, vec2):\n","  vec = vec1 - vec2\n","  norm = np.sum(np. power(vec,2))\n","  l2 = (norm * len(vec))/250\n","  return l2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iN1V30Es0vmo"},"source":["\n","\n","---\n","\n","\n","Analogy 1\n","\n","---\n","\n","> Checking the logic,\n","\n","> princess = prince - boy + girl\n","\n"]},{"cell_type":"code","metadata":{"id":"3b4-DjKKvhqt"},"source":["# Analogy 1\n","# for 50 length vector embedding\n","\n","princess_50_actual = myDict_50['princess']\n","princess_50_predicted = myDict_50['prince'] - myDict_50['boy'] + myDict_50['girl'] \n","loss_metric_50 = takeL2norm(princess_50_actual, princess_50_predicted)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c39rn8z1qEWW"},"source":["# for 300 length vector embedding\n","\n","princess_200_actual = myDict_200['princess']\n","princess_200_predicted = myDict_200['prince'] - myDict_200['boy'] + myDict_200['girl'] \n","loss_metric_200 = takeL2norm(princess_200_actual, princess_200_predicted)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mo7exD67vj4T","outputId":"6831bebc-ef7e-4fc3-d8c2-82803892ef93","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(loss_metric_50)\n","print(loss_metric_200)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.1125659543968114\n","23.508401588223208\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x3Zsu7rt0m3M"},"source":["\n","\n","---\n","\n","\n","Analogy 2\n","\n","---\n","\n","\n","\n",">Checking the logic,\n","\n",">asia**:**india **::** europe**:**germany"]},{"cell_type":"code","metadata":{"id":"VPdQ2M2MwIds"},"source":["# Analogy 2\n","# for 50 length embedding\n","\n","asia_50 = myDict_50['asia']\n","india_50 = myDict_50['india']\n","europe_50 = myDict_50['europe']\n","germany_50_actual = myDict_50['germany']\n","germany_50_predicted = europe_50 - asia_50 + india_50\n","loss_metric_50= takeL2norm(germany_50_actual, germany_50_predicted)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rMwtqqPxz3U"},"source":["# for 200 length embedding\n","\n","asia_200 = myDict_200['asia']\n","india_200 = myDict_200['india']\n","europe_200 = myDict_200['europe']\n","germany_200_actual = myDict_200['germany']\n","germany_200_predicted = europe_200 - asia_200 + india_200\n","loss_metric_200= takeL2norm(germany_200_actual, germany_200_predicted)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZ2KIPMCyoPE","outputId":"f6350f37-ca00-4de1-82a3-25b333b6a12b","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(loss_metric_50)\n","print(loss_metric_200)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5.26106093921469\n","41.86019923530512\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QNqEPGie0eTx"},"source":["\n","\n","---\n","\n","\n","Analogy 3\n","\n","---\n","\n","\n","\n",">Checking the logic,\n","\n",">do**:**did  **::** go**:**went"]},{"cell_type":"code","metadata":{"id":"ixpN_1meyWCP"},"source":["# Analogy 3\n","\n","# for 50 length embedding\n","\n","went_50_actual = myDict_50['went']\n","went_50_predicted = myDict_50['did'] - myDict_50['do'] + myDict_50['go']\n","loss_metric_50 = takeL2norm(went_50_actual, went_50_predicted)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4QpdZ0Z1yrv"},"source":["# for 200 length embedding\n","\n","went_200_actual = myDict_200['went']\n","went_200_predicted = myDict_200['did'] - myDict_200['do'] + myDict_200['go']\n","loss_metric_200 = takeL2norm(went_200_actual, went_200_predicted)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5I6r2j616Y6","outputId":"dab5d93e-3599-48d7-ba7e-77b32c4a5567","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(loss_metric_50)\n","print(loss_metric_200)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6423520522060218\n","8.688189643888883\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4LhWkCN51-Ml"},"source":["\n","---\n","> OBSERVATION\n","---\n","\n","> It is observed that smaller the length of embedding,lower the L2 norm, better the analogy. We can conclude that analytically close words when represented in lower dimensions would lie nearer."]}]}